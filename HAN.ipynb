{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQHT1Eq3Lcgl2HqeDiQoO1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaekyoungkim/NLP_STUDY/blob/main/HAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5cUxpwNzdm5"
      },
      "outputs": [],
      "source": [
        "# https://blog.naver.com/hist0134/221386940063\n",
        "# http://hist0134.blog.me/221179965199\n",
        "# https://github.com/hist0613/keras-implementations/blob/main/IMDB-HieAtt.ipynb\n",
        "# [Keras] Hierarchical Attention Networks for Document Classification 구현"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMDB-HieAtt.ipynb\n",
        "# We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. \n",
        "# "
      ],
      "metadata": {
        "id": "G27Jxk2L02cN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nltk.download('punkt')\n",
        "import nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8ZPKbBo53aY",
        "outputId": "859346e6-c1b0-4820-c5d2-7da5bf2126dd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `'punkt''\n",
            "/bin/bash: -c: line 0: `nltk.download('punkt')'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#4/1AX4XfWgKzT5FnEsEqbHv_MJeAYGD6bx-GaC_4wW5f-wWQPUEX9zI2zEL4ug\n",
        "# crm = pd.read_csv('gdrive/MyDrive/DW_DATA/CRMDATA_product.csv', encoding = 'c==p949')  # cp949\n",
        "# CRM품목정리 파일 활용\n",
        "# utf-8  or cp 949 로 코딩"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYSo3Sc3tkmX",
        "outputId": "f016b400-6abc-47f9-cfcf-6c2b88ecadc6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SENTENCES = 10\n",
        "MAX_SENTENCE_LENGTH = 25"
      ],
      "metadata": {
        "id": "kXxG-aZQ6Sbt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# refer: http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "data_dir = \"gdrive/MyDrive/dataset/IMDB/aclImdb\"  # 데이터 받아서 해당 폴더 생성한거에 집어넣음\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "print(train_dir)\n",
        "print(test_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPgMXfp1trmI",
        "outputId": "b4d22dd4-56e4-4020-c88a-b9532020b7ee"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gdrive/MyDrive/dataset/IMDMB/aclImdb/train\n",
            "gdrive/MyDrive/dataset/IMDMB/aclImdb/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_dir = os.path.join(base_dir, 'train')\n",
        "split_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ftjDAdsw1CLM",
        "outputId": "090fe270-22b4-4e4c-881f-51d959904f02"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gdrive/MyDrive/dataset/IMDMB/aclImdb/train'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(split='train'):\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "    \n",
        "    base_dir = 'gdrive/MyDrive/dataset/IMDB/aclImdb'\n",
        "    split_dir = os.path.join(base_dir, split)\n",
        "    for sentiment, y in [('neg', 0), ('pos', 1)]:\n",
        "        data_dir = os.path.join(split_dir, sentiment)\n",
        "        for file_name in os.listdir(data_dir):\n",
        "            file_path = os.path.join(data_dir, file_name)\n",
        "            with open(file_path, 'r', encoding='utf-8') as fp:\n",
        "                review = fp.read()\n",
        "            x_data.append(review)\n",
        "            y_data.append(y)\n",
        "            \n",
        "    return x_data, y_data"
      ],
      "metadata": {
        "id": "0qXBLIJl8hy3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_data, train_y_data = load_dataset(split='train')\n",
        "test_x_data, test_y_data = load_dataset(split='test')\n",
        "\n",
        "print(\"len(train_x_data): {}\".format(len(train_x_data)))\n",
        "print(\"len(test_x_data): {}\".format(len(test_x_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxlNkpgdz5Ef",
        "outputId": "184f67b7-a4a7-4aaa-a4e7-ddbbc21f7e6e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_x_data): 25000\n",
            "len(test_x_data): 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_data[5]"
      ],
      "metadata": {
        "id": "2-ZQVa4l6SfF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "220d9b29-0ede-43a3-f7ca-56708415cd51"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Are you kidding me?! A show highlighting someone who opens cans and envelopes for a meal? How talented do you have to be to do this? She MAY be able to cook but it is NOT portrayed in this half-hour stomach churning painful production. I know she has a Martha-Stewart-esquire empire. So does Warren Buffett but I don\\'t see him with fake knockers opening cans of cream corn and Alpo.<br /><br />She has a nephew named...Brycer. Brycer? Stop talking about anyone a name that stupid.<br /><br />More time is spent on \"table-scapes\" than actual cooking. Who has that kind of time?! Silicon should be on your spatula, not on my TV. This show should be on Cartoon Network, NOT Food Network.'"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y_data[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71uPbeN96ZcY",
        "outputId": "8319065d-2420-4b08-a30b-0df07d7c09f9"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer  # 토큰화하는 패키지\n",
        "from keras.preprocessing.sequence import pad_sequences # padding 을 위한 패키지\n",
        "#from keras.utils import to_categorical # 범주형 변수로 만들어줌 / 실행안되는 이유확인\n",
        "from tensorflow.keras.utils import to_categorical # \n",
        "from nltk.tokenize import sent_tokenize  # 문장을 token화 시킴\n"
      ],
      "metadata": {
        "id": "EF3wKKPp6Zfd"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=sent_tokenize(train_x_data[1])"
      ],
      "metadata": {
        "id": "rpqPvtQmIMov"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a  #문장단위로 끊어줌, 끊는 기준이 있을것임"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgsLVMnLINYu",
        "outputId": "f6cc9d4a-0fa4-4b9d-9e10-459f9acf6773"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['If i could have rated this movie by 0 i would have !',\n",
              " 'I see some ppl at IMDb says that this is the funniest movie of the year , etc etc excuse me ?',\n",
              " 'are you ppl snorting LSD or ........?',\n",
              " 'There is absolutely NOTHING funny about this movie N O T H I N G !',\n",
              " 'I actually want my 27 minutes back of my life that i spent watching this piece of crap.',\n",
              " '<br /><br />I read someone sitting on an airplane watching this movie stopped watching after 30 minutes , i totally understand that , i actually would have watched snakes on a plane for 2 times over instead of watching this movie once !',\n",
              " '<br /><br />DO NOT watch this movie , do something else useful with your life do the dishes , walk the dog , hell... anything is better than spending time in front of the TV watching hot rod.']"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = tokenizer.texts_to_sequences(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiIZ8soDIRuq",
        "outputId": "3ff5d07b-ac0b-45c4-c4d2-6dde18a2933a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[43, 10, 98, 25, 1196, 11, 17, 31, 2196, 10, 58, 25], [10, 63, 47, 18881, 30, 869, 546, 12, 11, 6, 1, 1536, 17, 4, 1, 301, 506, 506, 1385, 68], [23, 22, 18881, 17516, 11799, 38], [46, 6, 419, 161, 152, 42, 11, 17, 3184, 1461, 789, 2150, 10, 3184, 1299], [10, 160, 178, 56, 7720, 228, 141, 4, 56, 114, 12, 10, 995, 147, 11, 412, 4, 590], [7, 7, 10, 339, 296, 1241, 20, 32, 3922, 147, 11, 17, 2339, 147, 100, 1086, 228, 10, 456, 386, 12, 10, 160, 58, 25, 287, 3202, 20, 3, 1545, 15, 230, 209, 121, 298, 4, 147, 11, 17, 281], [7, 7, 77, 21, 103, 11, 17, 77, 137, 329, 4463, 16, 125, 114, 77, 1, 13497, 1192, 1, 826, 603, 231, 6, 126, 71, 3281, 55, 8, 982, 4, 1, 240, 147, 879, 5479]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = pad_sequences(b, maxlen=25) # 길이를 25로 맞추기\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXuT3uerLQ_e",
        "outputId": "52256f03-8d1a-4680-946a-1926c3c38590"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0    43    10    98    25  1196    11    17    31  2196    10    58\n",
            "     25]\n",
            " [    0     0     0     0     0    10    63    47 18881    30   869   546\n",
            "     12    11     6     1  1536    17     4     1   301   506   506  1385\n",
            "     68]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0    23    22 18881 17516 11799\n",
            "     38]\n",
            " [    0     0     0     0     0     0     0     0     0     0    46     6\n",
            "    419   161   152    42    11    17  3184  1461   789  2150    10  3184\n",
            "   1299]\n",
            " [    0     0     0     0     0     0     0    10   160   178    56  7720\n",
            "    228   141     4    56   114    12    10   995   147    11   412     4\n",
            "    590]\n",
            " [ 1086   228    10   456   386    12    10   160    58    25   287  3202\n",
            "     20     3  1545    15   230   209   121   298     4   147    11    17\n",
            "    281]\n",
            " [ 4463    16   125   114    77     1 13497  1192     1   826   603   231\n",
            "      6   126    71  3281    55     8   982     4     1   240   147   879\n",
            "   5479]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4w_E22KaLSFA"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bU1zycBLSLu",
        "outputId": "d73488a0-8d58-459f-c824-954afea3a11f"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    \n",
        "\n",
        "    pad_size = max_sentences - tokenized_sentences.shape[0]\n",
        "\n",
        "    if pad_size <= 0:  # tokenized_sentences.shape[0] < max_sentences\n",
        "        tokenized_sentences = tokenized_sentences[:max_sentences]\n",
        "    else:\n",
        "        tokenized_sentences = np.pad(\n",
        "            tokenized_sentences, ((0, pad_size), (0, 0)),\n",
        "            mode='constant', constant_values=0\n",
        "        )"
      ],
      "metadata": {
        "id": "ARbAOFnXK60s"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fu3-5T3NK64a"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "URcrZRWnK67V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_x_data)\n",
        "tokenizer.fit_on_texts(test_x_data)\n",
        "\n",
        "max_nb_words = len(tokenizer.word_index) + 1  # 토큰나이즈했을때에 나타나는 단어개수만큼\n"
      ],
      "metadata": {
        "id": "44Qxt8TtGPRv"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg8Vim0w-iQ9",
        "outputId": "e71b62e8-1737-4fb0-d6ee-80108d286b8c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_preprocessing.text.Tokenizer at 0x7f8a1730bb50>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_nb_words # 124253"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yx26Kll-lJw",
        "outputId": "22e65806-7951-439d-d530-7f833aa62d40"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124253"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SENTENCE_LENGTH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FexyIekKHLrc",
        "outputId": "a78a194e-2a37-4901-80c7-84d6825957dc"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SENTENCES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCopOJnuHMiT",
        "outputId": "a53eaf61-a74b-43c3-949e-0adb6351c18a"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpdtuVdR-yFa",
        "outputId": "fdf7248b-0629-4ece-b5dd-d6272cd80177"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()\n",
        "# 여기서 punkt를 설치할 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLHYU4z_FkQw",
        "outputId": "7f9d63ac-b7cf-483a-a58a-62b55300b439"
      },
      "execution_count": 66,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n",
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def doc2hierarchical(text,\n",
        "                     max_sentences=MAX_SENTENCES,\n",
        "                     max_sentence_length=MAX_SENTENCE_LENGTH):\n",
        "    sentences = sent_tokenize(text)  \n",
        "    tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "    tokenized_sentences = pad_sequences(tokenized_sentences, maxlen=max_sentence_length)\n",
        "\n",
        "    pad_size = max_sentences - tokenized_sentences.shape[0]\n",
        "\n",
        "    if pad_size <= 0:  # tokenized_sentences.shape[0] < max_sentences\n",
        "        tokenized_sentences = tokenized_sentences[:max_sentences]\n",
        "    else:\n",
        "        tokenized_sentences = np.pad(\n",
        "            tokenized_sentences, ((0, pad_size), (0, 0)),\n",
        "            mode='constant', constant_values=0\n",
        "        )\n",
        "    \n",
        "    return tokenized_sentences\n",
        "            \n",
        "def build_dataset(x_data, y_data, \n",
        "                  max_sentences=MAX_SENTENCES, \n",
        "                  max_sentence_length=MAX_SENTENCE_LENGTH,\n",
        "                  tokenizer=tokenizer):\n",
        "    \n",
        "    nb_instances = len(x_data)\n",
        "    X_data = np.zeros((nb_instances, max_sentences, max_sentence_length), dtype='int32')\n",
        "    for i, review in enumerate(x_data):\n",
        "        tokenized_sentences = doc2hierarchical(review)\n",
        "            \n",
        "        X_data[i] = tokenized_sentences[None, ...]\n",
        "        \n",
        "    nb_classes = len(set(y_data))  # 0,1 2개 카테고리 > 2값이 나옴\n",
        "    Y_data = to_categorical(y_data, nb_classes)  #  1/0 binary를 2개의 컬럼으로 나타냄\n",
        "    \n",
        "    return X_data, Y_data\n",
        "\n",
        "train_X_data, train_Y_data = build_dataset(train_x_data, train_y_data)\n",
        "test_X_data, test_Y_data = build_dataset(test_x_data, test_y_data)\n",
        "\n",
        "print(\"train_X_data.shape: {}\".format(train_X_data.shape))\n",
        "print(\"test_X_data.shape: {}\".format(test_X_data.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCN3o4KM5HrD",
        "outputId": "f026461c-9598-4de8-f249-8a87d81efd37"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X_data.shape: (25000, 10, 25)\n",
            "test_X_data.shape: (25000, 10, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_X_data, val_X_data, train_Y_data, val_Y_data = train_test_split(train_X_data, train_Y_data, \n",
        "                                                                      test_size=0.1, \n",
        "                                                                      random_state=42)\n",
        "print(\"train_X_data.shape: {}\".format(train_X_data.shape))\n",
        "print(\"train_Y_data.shape: {}\".format(train_Y_data.shape))\n",
        "print(\"val_X_data.shape: {}\".format(val_X_data.shape))\n",
        "print(\"val_Y_data.shape: {}\".format(val_Y_data.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNcKOXyA6KDb",
        "outputId": "8bed0440-373d-4268-c0d7-1dc0ccb72f72"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X_data.shape: (22500, 10, 25)\n",
            "train_Y_data.shape: (22500, 2)\n",
            "val_X_data.shape: (2500, 10, 25)\n",
            "val_Y_data.shape: (2500, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X_data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S16lt544GtY3",
        "outputId": "038f226a-0ca4-419a-f64e-e547b6e63232"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    0,     0,     0,    56,   325,     2,    10,    37,     5,\n",
              "          817,    62,   364,   182,   859,   861,    97,     2,   103,\n",
              "           93,    16,   260,   350,    15,     3,   423],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           73,   210,    11,    27,    20, 83582,  2295,     2,   877,\n",
              "            5,   726,     9,     5,   260,  6526,  1011],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,   146,    50,    10,   131,    11,    17,     6,   372,\n",
              "           10,   383,     9,     8,     3,    49,    95],\n",
              "       [    0,     0,   267,    42,     9,     1,   113,   355,   158,\n",
              "           64,  1367,     6,    39,    34, 12595,     2,   359,   332,\n",
              "           18,  1606,    48,   162,     9,    34,    49],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,    10,   101,     8,    27,   129,     1,   150,   588,\n",
              "           37,    33,    70,   160,  1334,     8,   274],\n",
              "       [    0,    10,    62,   433,    12,  2590,    90,    11,    19,\n",
              "          292,   600,    50,    33,    90,     9,    84,    43,    33,\n",
              "           70,    91,    12,    58,    39,    26,   625],\n",
              "       [    0,     0,     0,     0,     0,    43,    22,    37,     5,\n",
              "          103,    62,   364,   182,    97,    39,     5,    94,   245,\n",
              "            4,    93,    91,    10,   377,    11,    27],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_Y_data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUE6CQP9G12x",
        "outputId": "bd5137c6-e574-415b-eb69-32d142d3a6bf"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# train , test 데이터 분리하기\n",
        "train_X_data, val_X_data, train_Y_data, val_Y_data = train_test_split(train_X_data, train_Y_data, \n",
        "                                                                      test_size=0.1, \n",
        "                                                                      random_state=42)\n",
        "\n",
        "print(\"train_X_data.shape: {}\".format(train_X_data.shape))\n",
        "print(\"train_Y_data.shape: {}\".format(train_Y_data.shape))\n",
        "print(\"val_X_data.shape: {}\".format(val_X_data.shape))\n",
        "print(\"val_Y_data.shape: {}\".format(val_Y_data.shape))"
      ],
      "metadata": {
        "id": "k8mLU7sU6Shr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0411abb3-a8e4-4032-8f8e-31960ff66446"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X_data.shape: (18225, 10, 25)\n",
            "train_Y_data.shape: (18225, 2)\n",
            "val_X_data.shape: (2025, 10, 25)\n",
            "val_Y_data.shape: (2025, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# google이 이미 word2vec의 결과로 각 word에 대한 vector를 만들어서 배포\n",
        "# 하지만 용량이 커서 작은 용량에서는 어렵죠. 그래서 작은 용량의 word2vec vector로 존재\n",
        "# vector를 특정 분야에 맞게 특화시키킬 원할 경우,\n",
        "# Word2vec.intersect_word2vec_format(googleNews_filepath, binary=True, lockf=1.0)을 통해 쉽게 초기값을 설정\n",
        "# word2vec-GoogleNews-vectors에서 이미 학습된 word2vector를 다운가능\n",
        "# 다만, 학습된 모델을 가져오는 것이 아니라, “학습된 vector”만을 수치로 가져오는 것이죠.\n",
        "\n",
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')\n",
        "vec_king = wv['king']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9rmGu6bQ3fR",
        "outputId": "ae02ee4c-56b7-4a03-bd45-97d4151b1310"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv['boy'].shape # 300차원 벡터로 표현됨  GoogleNews-vectors-negative300"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xcuC-1HTuid",
        "outputId": "782b309a-fe90-41e8-aa9a-f468948033dd"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dir = 'gdrive/MyDrive/dataset/word2vec'\n",
        "# word2vec을 가져옴\n",
        "def load_word2vec(tokenizer=tokenizer):\n",
        "    from gensim.models import KeyedVectors\n",
        "    embedding_path = os.path.join(embedding_dir, 'GoogleNews-vectors-negative300.bin')\n",
        "    # https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz 해당링크에서 파일 다운로드 1.5GB정도됨\n",
        "    embeddings_index = KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
        "    \n",
        "    return embeddings_index\n",
        "    "
      ],
      "metadata": {
        "id": "Uz4dJkL6_VR4"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embedding(embedding_type='word2vec',\n",
        "                   tokenizer=tokenizer,\n",
        "                   embedding_dim=300):\n",
        "    \n",
        "    if embedding_type == 'word2vec':\n",
        "        embeddings_index = load_word2vec()\n",
        "        \n",
        "    embedding_matrix = np.random.normal(0, 1, (max_nb_words, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        try:\n",
        "            embedding_vector = embeddings_index[word]\n",
        "        except KeyError:\n",
        "            embedding_vector = None\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    return embedding_matrix\n"
      ],
      "metadata": {
        "id": "bHkCVzI4TumR"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = load_embedding('word2vec')\n",
        "\n",
        "print(\"embedding_matrix.shape: {}\".format(embedding_matrix.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55Hd3H1WZynu",
        "outputId": "e0274bdc-7e3e-40d1-da33-33c4bbd7c680"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding_matrix.shape: (124253, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix # 총 124253 개의 단어들을 각각 300차원으로 만들어서 표현함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFKP4bYbacNr",
        "outputId": "cf839283-06d4-496d-9663-62532b93ae70"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.24844742, -0.86570778,  0.41408744, ...,  0.55752305,\n",
              "         0.02716441, -1.35658199],\n",
              "       [ 0.08007812,  0.10498047,  0.04980469, ...,  0.00366211,\n",
              "         0.04760742, -0.06884766],\n",
              "       [-0.29843929, -0.12018323, -0.33024155, ...,  1.22457275,\n",
              "        -0.31415279,  3.06416716],\n",
              "       ...,\n",
              "       [-0.9281386 , -1.03073501, -0.50314152, ...,  0.17111687,\n",
              "        -0.09080699, -0.39828176],\n",
              "       [-0.09130859, -0.28320312,  0.07128906, ..., -0.33789062,\n",
              "        -0.36132812,  0.29101562],\n",
              "       [-0.18066406,  0.10351562, -0.0324707 , ...,  0.19238281,\n",
              "        -0.14550781,  0.0480957 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "#from keras.engine.topology import Layer 수정해야함\n",
        "from tensorflow.python.keras.layers import Layer, InputSpec\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "\n",
        "\n",
        "from keras.layers import Input, Embedding, Dense\n",
        "from keras.layers import Lambda, Permute, RepeatVector, Multiply\n",
        "from keras.layers import Bidirectional, TimeDistributed\n",
        "from keras.layers import CuDNNGRU\n",
        "from keras.layers import BatchNormalization, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n"
      ],
      "metadata": {
        "id": "LoiA1TDobNdd"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(Layer):  # Layer를 가져옴\n",
        "    def __init__(self, attention_dim, **kwargs):\n",
        "        self.attention_dim = attention_dim\n",
        "        super(AttentionLayer, self).__init__(**kwargs)  # attentionlayer의 속성을 가져옴\n",
        "    \n",
        "    def build(self, input_shape):  \n",
        "        self.W = self.add_weight(name='Attention_Weight', # attention 값 이값을 확인해야하는것 같음\n",
        "                                 shape=(input_shape[-1], self.attention_dim),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)  # 학습가능\n",
        "        self.b = self.add_weight(name='Attention_Bias',\n",
        "                                 shape=(self.attention_dim, ), # attention값과 쌍을 이룸?\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        self.u = self.add_weight(name='Attention_Context_Vector',  # u: context vector\n",
        "                                 shape=(self.attention_dim, 1),  # attention값과 쌍을 이룸?\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "        \n",
        "    def call(self, x):\n",
        "        # refer to the original paper\n",
        "        # link: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
        "        u_it = K.tanh(K.dot(x, self.W) + self.b)\n",
        "        a_it = K.dot(u_it, self.u)\n",
        "        a_it = K.squeeze(a_it, -1)\n",
        "        a_it = K.softmax(a_it)\n",
        "        \n",
        "        return a_it\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1])\n",
        "    \n",
        "\n",
        "def WeightedSum(attentions, representations):\n",
        "    # from Shape(batch_size, len_units) to Shape(batch_size, rnn_dim * 2, len_units)\n",
        "    repeated_attentions = RepeatVector(K.int_shape(representations)[-1])(attentions)\n",
        "    # from Shape(batch_size, rnn_dim * 2, len_units) to Shape(batch_size, len_units, lstm_dim * 2)\n",
        "    repeated_attentions = Permute([2, 1])(repeated_attentions)\n",
        "\n",
        "    # compute representation as the weighted sum of representations\n",
        "    aggregated_representation = Multiply()([representations, repeated_attentions])\n",
        "    aggregated_representation = Lambda(lambda x: K.sum(x, axis=1))(aggregated_representation)\n",
        "\n",
        "    return aggregated_representation\n",
        "    \n",
        "    \n",
        "def HieAtt(embedding_matrix,\n",
        "           max_sentences,\n",
        "           max_sentence_length,\n",
        "           nb_classes,\n",
        "           embedding_dim=300,\n",
        "           attention_dim=100,\n",
        "           rnn_dim=150,\n",
        "           include_dense_batch_normalization=False,\n",
        "           include_dense_dropout=True,\n",
        "           nb_dense=1,\n",
        "           dense_dim=300,\n",
        "           dense_dropout=0.2,\n",
        "           optimizer = tf.keras.optimizers.Adam(lr=0.001)):\n",
        "# Use tf.keras.optimizers.Adam(learning_rate) instead of keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "    # embedding_matrix = (max_nb_words + 1, embedding_dim)\n",
        "    max_nb_words = embedding_matrix.shape[0] - 1\n",
        "    embedding_layer = Embedding(max_nb_words + 1, \n",
        "                                embedding_dim,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=max_sentence_length,\n",
        "                                trainable=False)\n",
        "\n",
        "    # first, build a sentence encoder\n",
        "    sentence_input = Input(shape=(max_sentence_length, ), dtype='int32')\n",
        "    embedded_sentence = embedding_layer(sentence_input)\n",
        "    embedded_sentence = Dropout(dense_dropout)(embedded_sentence)\n",
        "    contextualized_sentence = Bidirectional(CuDNNGRU(rnn_dim, return_sequences=True))(embedded_sentence)\n",
        "    \n",
        "    # word attention computation\n",
        "    word_attention = AttentionLayer(attention_dim)(contextualized_sentence)\n",
        "    sentence_representation = WeightedSum(word_attention, contextualized_sentence)\n",
        "    \n",
        "    sentence_encoder = Model(inputs=[sentence_input], \n",
        "                             outputs=[sentence_representation])\n",
        "\n",
        "    # then, build a document encoder\n",
        "    document_input = Input(shape=(max_sentences, max_sentence_length), dtype='int32')\n",
        "    embedded_document = TimeDistributed(sentence_encoder)(document_input)\n",
        "    contextualized_document = Bidirectional(CuDNNGRU(rnn_dim, return_sequences=True))(embedded_document)\n",
        "    \n",
        "    # sentence attention computation\n",
        "    sentence_attention = AttentionLayer(attention_dim)(contextualized_document)\n",
        "    document_representation = WeightedSum(sentence_attention, contextualized_document)\n",
        "    \n",
        "    # finally, add fc layers for classification\n",
        "    fc_layers = Sequential()\n",
        "    for _ in range(nb_dense):\n",
        "        if include_dense_batch_normalization == True:\n",
        "            fc_layers.add(BatchNormalization())\n",
        "        fc_layers.add(Dense(dense_dim, activation='relu'))\n",
        "        if include_dense_dropout == True:\n",
        "            fc_layers.add(Dropout(dense_dropout))\n",
        "    fc_layers.add(Dense(nb_classes, activation='softmax'))\n",
        "    \n",
        "    pred_sentiment = fc_layers(document_representation)\n",
        "\n",
        "    model = Model(inputs=[document_input],\n",
        "                  outputs=[pred_sentiment])\n",
        "    \n",
        "    ############### build attention extractor ###############\n",
        "    word_attention_extractor = Model(inputs=[sentence_input],\n",
        "                                     outputs=[word_attention])\n",
        "    word_attentions = TimeDistributed(word_attention_extractor)(document_input)\n",
        "    attention_extractor = Model(inputs=[document_input],\n",
        "                                     outputs=[word_attentions, sentence_attention])\n",
        "    \n",
        "    model.compile(loss=['categorical_crossentropy'],\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    return model, attention_extractor\n"
      ],
      "metadata": {
        "id": "0bpE5B-9_VVK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6449f35f-e145-4a37-cbee-f23c2023e2e9"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model_name = \"HieAtt\"\n",
        "model_path = './models/checkpoints/{}.h5'.format(model_name)\n",
        "checkpointer = ModelCheckpoint(filepath=model_path,\n",
        "                               monitor='val_acc',\n",
        "                               verbose=True,\n",
        "                               save_best_only=True,\n",
        "                               mode='max')\n",
        "\n",
        "model, attention_extractor = HieAtt(embedding_matrix=embedding_matrix,\n",
        "                                    max_sentences=MAX_SENTENCES,\n",
        "                                    max_sentence_length=MAX_SENTENCE_LENGTH,\n",
        "                                    nb_classes=2,\n",
        "                                    embedding_dim=300,\n",
        "                                    attention_dim=100,\n",
        "                                    rnn_dim=150,\n",
        "                                    include_dense_batch_normalization=False,\n",
        "                                    include_dense_dropout=True,\n",
        "                                    nb_dense=1,\n",
        "                                    dense_dim=300,\n",
        "                                    dense_dropout=0.2,\n",
        "                                    optimizer = tf.keras.optimizers.Adam(lr=0.001))\n",
        "\n",
        "history = model.fit(x=[train_X_data],\n",
        "                    y=[train_Y_data],\n",
        "                    batch_size=128,\n",
        "                    epochs=30,\n",
        "                    verbose=True,\n",
        "                    validation_data=(val_X_data, val_Y_data),\n",
        "                    callbacks=[checkpointer])"
      ],
      "metadata": {
        "id": "owsfhXOzeYmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(model_path)\n",
        "score = model.evaluate(test_X_data, test_Y_data, verbose=0, batch_size=128)\n",
        "print(\"Test Accuracy of {}: {}\".format(model_name, score[1]))"
      ],
      "metadata": {
        "id": "eimpQWvt_VYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B4GLYO136SkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q83837Y4_dhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "GTsqYBoB_dkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sn\n",
        "\n",
        "word_rev_index = {}\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    word_rev_index[i] = word\n",
        "\n",
        "def sentiment_analysis(review):        \n",
        "    tokenized_sentences = doc2hierarchical(review)\n",
        "    \n",
        "    # word attention만 가져오기\n",
        "    pred_attention = attention_extractor.predict(np.asarray([tokenized_sentences]))[0][0]\n",
        "    for sent_idx, sentence in enumerate(tokenized_sentences):\n",
        "        if sentence[-1] == 0:\n",
        "            continue\n",
        "            \n",
        "        for word_idx in range(MAX_SENTENCE_LENGTH):\n",
        "            if sentence[word_idx] != 0:\n",
        "                words = [word_rev_index[word_id] for word_id in sentence[word_idx:]]\n",
        "                pred_att = pred_attention[sent_idx][-len(words):]\n",
        "                pred_att = np.expand_dims(pred_att, axis=0)\n",
        "                break\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(len(words), 1))\n",
        "        plt.rc('xtick', labelsize=16)\n",
        "        midpoint = (max(pred_att[:, 0]) - min(pred_att[:, 0])) / 2\n",
        "        heatmap = sn.heatmap(pred_att, xticklabels=words, yticklabels=False, square=True, linewidths=0.1, cmap='coolwarm', center=midpoint, vmin=0, vmax=1)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.show()\n",
        "        \n",
        "# sentiment_analysis(\"Delicious healthy food. The steak is amazing. Fish and pork are awesome too. Service is above and beyond. Not a bad thing to say about this place. Worth every penny!\")\n",
        "sentiment_analysis(\"Absolute perfection end game !! Good acting performance to all the characters. Great cgi's. Truly epic & perfect ending to a long journey of marvel movie. Go "
      ],
      "metadata": {
        "id": "376HU-uc_dnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XO6gj4Ot_dq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kj8023Ao6Smy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document classification에 사용할 데이터셋은 Yelp Reviews 입니다. sentiment classification을 하는 데이터셋인데, 각 줄마다 하나의 json 데이터가 있는 구조\n",
        "x_train = []\n",
        "y_train = []\n",
        "with open(data_dir, 'r', encoding=\"utf-8\") as fp:\n",
        "    for line in fp:\n",
        "        # ['user_id', 'stars', 'text', 'funny', 'useful', 'review_id', 'business_id', 'date', 'cool']\n",
        "        line = json.loads(line)\n",
        "        x_train.append(line['text'])\n",
        "        y_train.append(int(line['stars']) - 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "qrGxYb5T02fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FAZXJPLR02h8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}